{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a8b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from scipy import signal\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1451f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../../data/william/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aff76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_folder = \"../../fig/william/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7174113",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = f'{data_folder}/preprocessed_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcfe066b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sitting</td>\n",
       "      <td>-0.416300</td>\n",
       "      <td>-0.417491</td>\n",
       "      <td>-0.420441</td>\n",
       "      <td>-0.429083</td>\n",
       "      <td>-0.420441</td>\n",
       "      <td>-0.414827</td>\n",
       "      <td>-0.414827</td>\n",
       "      <td>-0.425872</td>\n",
       "      <td>-0.434747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549077</td>\n",
       "      <td>0.555241</td>\n",
       "      <td>0.555241</td>\n",
       "      <td>0.539026</td>\n",
       "      <td>0.500123</td>\n",
       "      <td>0.539026</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>0.541752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sitting</td>\n",
       "      <td>-0.975244</td>\n",
       "      <td>-0.975244</td>\n",
       "      <td>-0.975244</td>\n",
       "      <td>-0.956182</td>\n",
       "      <td>-0.958875</td>\n",
       "      <td>-0.958875</td>\n",
       "      <td>-0.958875</td>\n",
       "      <td>-0.939893</td>\n",
       "      <td>-0.934297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424881</td>\n",
       "      <td>1.448575</td>\n",
       "      <td>1.448575</td>\n",
       "      <td>1.456614</td>\n",
       "      <td>1.460817</td>\n",
       "      <td>1.460817</td>\n",
       "      <td>1.464875</td>\n",
       "      <td>1.485612</td>\n",
       "      <td>1.485612</td>\n",
       "      <td>1.419242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sitting</td>\n",
       "      <td>-0.075665</td>\n",
       "      <td>-0.075665</td>\n",
       "      <td>-0.031610</td>\n",
       "      <td>-0.031610</td>\n",
       "      <td>-0.044277</td>\n",
       "      <td>-0.054205</td>\n",
       "      <td>-0.054205</td>\n",
       "      <td>-0.038245</td>\n",
       "      <td>-0.038245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.450913</td>\n",
       "      <td>-0.458894</td>\n",
       "      <td>-0.458894</td>\n",
       "      <td>-0.437464</td>\n",
       "      <td>-0.429559</td>\n",
       "      <td>-0.429559</td>\n",
       "      <td>-0.465102</td>\n",
       "      <td>-0.465102</td>\n",
       "      <td>-0.437855</td>\n",
       "      <td>-0.390392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sitting</td>\n",
       "      <td>-1.344867</td>\n",
       "      <td>-1.358563</td>\n",
       "      <td>-1.358563</td>\n",
       "      <td>-1.358563</td>\n",
       "      <td>-1.360337</td>\n",
       "      <td>-1.360337</td>\n",
       "      <td>-1.400246</td>\n",
       "      <td>-1.406819</td>\n",
       "      <td>-1.400246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714682</td>\n",
       "      <td>1.714682</td>\n",
       "      <td>1.714712</td>\n",
       "      <td>1.742487</td>\n",
       "      <td>1.742487</td>\n",
       "      <td>1.713822</td>\n",
       "      <td>1.713822</td>\n",
       "      <td>1.737701</td>\n",
       "      <td>1.737701</td>\n",
       "      <td>1.730101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sitting</td>\n",
       "      <td>-0.972655</td>\n",
       "      <td>-1.007781</td>\n",
       "      <td>-1.018223</td>\n",
       "      <td>-1.018223</td>\n",
       "      <td>-1.061634</td>\n",
       "      <td>-1.061634</td>\n",
       "      <td>-1.059564</td>\n",
       "      <td>-1.026008</td>\n",
       "      <td>-1.026008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.947539</td>\n",
       "      <td>1.947539</td>\n",
       "      <td>1.947539</td>\n",
       "      <td>1.883424</td>\n",
       "      <td>1.903985</td>\n",
       "      <td>1.903985</td>\n",
       "      <td>1.903985</td>\n",
       "      <td>1.918113</td>\n",
       "      <td>1.909350</td>\n",
       "      <td>1.909350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>standing</td>\n",
       "      <td>0.362166</td>\n",
       "      <td>0.462288</td>\n",
       "      <td>0.467350</td>\n",
       "      <td>0.467350</td>\n",
       "      <td>0.441121</td>\n",
       "      <td>0.441121</td>\n",
       "      <td>0.441121</td>\n",
       "      <td>0.443292</td>\n",
       "      <td>0.443292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600820</td>\n",
       "      <td>-0.605646</td>\n",
       "      <td>-0.622196</td>\n",
       "      <td>-0.622196</td>\n",
       "      <td>-0.637492</td>\n",
       "      <td>-0.597702</td>\n",
       "      <td>-0.637492</td>\n",
       "      <td>-0.597702</td>\n",
       "      <td>-0.642326</td>\n",
       "      <td>-0.506493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>standing</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>-0.020282</td>\n",
       "      <td>-0.013312</td>\n",
       "      <td>-0.016767</td>\n",
       "      <td>-0.005698</td>\n",
       "      <td>-0.016767</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>-0.003027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071983</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>0.046967</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.060659</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.098476</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>-0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>standing</td>\n",
       "      <td>0.945325</td>\n",
       "      <td>0.952541</td>\n",
       "      <td>0.962438</td>\n",
       "      <td>0.962438</td>\n",
       "      <td>0.950656</td>\n",
       "      <td>0.927253</td>\n",
       "      <td>0.927253</td>\n",
       "      <td>0.976097</td>\n",
       "      <td>0.980584</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.318418</td>\n",
       "      <td>-1.337065</td>\n",
       "      <td>-1.371594</td>\n",
       "      <td>-1.372040</td>\n",
       "      <td>-1.372942</td>\n",
       "      <td>-1.372040</td>\n",
       "      <td>-1.372942</td>\n",
       "      <td>-1.362027</td>\n",
       "      <td>-1.374201</td>\n",
       "      <td>-1.351678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>standing</td>\n",
       "      <td>1.015584</td>\n",
       "      <td>1.015584</td>\n",
       "      <td>1.015823</td>\n",
       "      <td>1.027837</td>\n",
       "      <td>1.027837</td>\n",
       "      <td>1.011747</td>\n",
       "      <td>1.008753</td>\n",
       "      <td>1.011747</td>\n",
       "      <td>1.024149</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.649722</td>\n",
       "      <td>-1.649722</td>\n",
       "      <td>-1.659491</td>\n",
       "      <td>-1.679571</td>\n",
       "      <td>-1.679571</td>\n",
       "      <td>-1.668293</td>\n",
       "      <td>-1.665690</td>\n",
       "      <td>-1.665690</td>\n",
       "      <td>-1.622155</td>\n",
       "      <td>-1.598536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>standing</td>\n",
       "      <td>0.634803</td>\n",
       "      <td>0.634803</td>\n",
       "      <td>0.616892</td>\n",
       "      <td>0.616892</td>\n",
       "      <td>0.610483</td>\n",
       "      <td>0.610483</td>\n",
       "      <td>0.591318</td>\n",
       "      <td>0.591318</td>\n",
       "      <td>0.591115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776435</td>\n",
       "      <td>-0.776435</td>\n",
       "      <td>-0.702657</td>\n",
       "      <td>-0.702657</td>\n",
       "      <td>-0.712741</td>\n",
       "      <td>-0.712741</td>\n",
       "      <td>-0.696611</td>\n",
       "      <td>-0.696611</td>\n",
       "      <td>-0.729471</td>\n",
       "      <td>-0.729471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label         0         1         2         3         4         5  \\\n",
       "0     sitting -0.416300 -0.417491 -0.420441 -0.429083 -0.420441 -0.414827   \n",
       "1     sitting -0.975244 -0.975244 -0.975244 -0.956182 -0.958875 -0.958875   \n",
       "2     sitting -0.075665 -0.075665 -0.031610 -0.031610 -0.044277 -0.054205   \n",
       "3     sitting -1.344867 -1.358563 -1.358563 -1.358563 -1.360337 -1.360337   \n",
       "4     sitting -0.972655 -1.007781 -1.018223 -1.018223 -1.061634 -1.061634   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "345  standing  0.362166  0.462288  0.467350  0.467350  0.441121  0.441121   \n",
       "346  standing -0.025810 -0.025810 -0.020282 -0.013312 -0.016767 -0.005698   \n",
       "347  standing  0.945325  0.952541  0.962438  0.962438  0.950656  0.927253   \n",
       "348  standing  1.015584  1.015584  1.015823  1.027837  1.027837  1.011747   \n",
       "349  standing  0.634803  0.634803  0.616892  0.616892  0.610483  0.610483   \n",
       "\n",
       "            6         7         8  ...      1990      1991      1992  \\\n",
       "0   -0.414827 -0.425872 -0.434747  ...  0.549077  0.555241  0.555241   \n",
       "1   -0.958875 -0.939893 -0.934297  ...  1.424881  1.448575  1.448575   \n",
       "2   -0.054205 -0.038245 -0.038245  ... -0.450913 -0.458894 -0.458894   \n",
       "3   -1.400246 -1.406819 -1.400246  ...  1.714682  1.714682  1.714712   \n",
       "4   -1.059564 -1.026008 -1.026008  ...  1.947539  1.947539  1.947539   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "345  0.441121  0.443292  0.443292  ... -0.600820 -0.605646 -0.622196   \n",
       "346 -0.016767  0.022315 -0.003027  ...  0.071983  0.046967  0.046967   \n",
       "347  0.927253  0.976097  0.980584  ... -1.318418 -1.337065 -1.371594   \n",
       "348  1.008753  1.011747  1.024149  ... -1.649722 -1.649722 -1.659491   \n",
       "349  0.591318  0.591318  0.591115  ... -0.776435 -0.776435 -0.702657   \n",
       "\n",
       "         1993      1994      1995      1996      1997      1998      1999  \n",
       "0    0.539026  0.500123  0.539026  0.543436  0.543436  0.543436  0.541752  \n",
       "1    1.456614  1.460817  1.460817  1.464875  1.485612  1.485612  1.419242  \n",
       "2   -0.437464 -0.429559 -0.429559 -0.465102 -0.465102 -0.437855 -0.390392  \n",
       "3    1.742487  1.742487  1.713822  1.713822  1.737701  1.737701  1.730101  \n",
       "4    1.883424  1.903985  1.903985  1.903985  1.918113  1.909350  1.909350  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "345 -0.622196 -0.637492 -0.597702 -0.637492 -0.597702 -0.642326 -0.506493  \n",
       "346  0.055400  0.055400  0.060659  0.084731  0.098476  0.084731 -0.000364  \n",
       "347 -1.372040 -1.372942 -1.372040 -1.372942 -1.362027 -1.374201 -1.351678  \n",
       "348 -1.679571 -1.679571 -1.668293 -1.665690 -1.665690 -1.622155 -1.598536  \n",
       "349 -0.702657 -0.712741 -0.712741 -0.696611 -0.696611 -0.729471 -0.729471  \n",
       "\n",
       "[350 rows x 2001 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f_name, index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "446e666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        'Initialization'\n",
    "        self.x = torch.from_numpy(x.copy()).float()\n",
    "        self.y = torch.from_numpy(y.copy()).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def get_data(data_folder):\n",
    "\n",
    "    f_name = f'{data_folder}/preprocessed_data.csv'\n",
    "    df = pd.read_csv(f_name, index_col=0)\n",
    "\n",
    "    idx_data = df.columns[1:]\n",
    "\n",
    "    labels = list(df.label.unique())\n",
    "    print(f\"N labels = {len(labels)}\")\n",
    "\n",
    "    # n_obs = len(df)\n",
    "    # n_feature = len(idx_data)\n",
    "\n",
    "    x = df[idx_data].values\n",
    "\n",
    "    x = signal.decimate(x, 4, axis=1)\n",
    "\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "    df.label = pd.Categorical(df.label)\n",
    "    y = df.label.cat.codes.values\n",
    "\n",
    "    print(\"number of label 0\", len(y) - y.sum())\n",
    "    print(\"number of label 1\", y.sum())\n",
    "\n",
    "    print(\"X shape\", x.shape)\n",
    "\n",
    "    training_data = Dataset(x, y)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f49a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N labels = 2\n",
      "standing    178\n",
      "sitting     172\n",
      "Name: label, dtype: int64\n",
      "number of label 0 172\n",
      "number of label 1 178\n",
      "X shape (350, 500)\n"
     ]
    }
   ],
   "source": [
    "train = get_data(data_folder=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d4ecd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_normal(mu, logvar, latent_dim):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    shape = (mu.size(0), latent_dim)\n",
    "    rn = torch.randn(shape)\n",
    "    z = rn * std + mu\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6bd26ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(512, latent_dim)\n",
    "        self.logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = self.model(x)\n",
    "        mu = self.mu(x_)\n",
    "        logvar = self.logvar(x_)\n",
    "        z = sample_normal(mu, logvar, latent_dim)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae4a9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9cc3ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        validity = self.model(z)\n",
    "        return validity.squeeze().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa271eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use binary cross-entropy loss\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "reconstruction_loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7efde14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 500\n",
      "latent_dim 3\n"
     ]
    }
   ],
   "source": [
    "input_dim = train.x.shape[1]\n",
    "latent_dim = 3\n",
    "print(\"input_dim\", input_dim)\n",
    "print(\"latent_dim\", latent_dim)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "discriminator = Discriminator(latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec4c722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "batch_size = len(train)\n",
    "dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70c3cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "lr = 0.005\n",
    "b1 = 0.3\n",
    "b2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(encoder.parameters(), decoder.parameters()), lr=lr, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39800b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n_row, batches_done, directory):\n",
    "    \"\"\"Saves a grid of generated digits\"\"\"\n",
    "    # Sample noise\n",
    "    z = torch.randn(latent_dim)\n",
    "    gen_x = decoder(z)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=n_row)\n",
    "    print(gen_x.data.shape)\n",
    "    raise Exception\n",
    "    #(gen_x.data, normalize=True)\n",
    "    \n",
    "    plt.savefig(f\"{directory}/{batches_done}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "755dd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e25cf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../../fig/william/aae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d74b1a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"binary_cross_entropy\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_y\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Loss measures generator's ability to fool the discriminator\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43madversarial_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m*\u001b[39m reconstruction_loss(decoded, real_imgs)\n\u001b[1;32m     20\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/torch/nn/modules/loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/lib/python3.9/site-packages/torch/nn/functional.py:3083\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3080\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3081\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"binary_cross_entropy\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        encoded = encoder(batch_x)\n",
    "        decoded = decoder(encoded)\n",
    "        predicted = discriminator(encoded)\n",
    "        print(predicted.squeeze().dtype)\n",
    "        print(batch_y.dtype)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = 0.001 * adversarial_loss(predicted.squeeze(), batch_y)\\\n",
    "            + 0.999 * reconstruction_loss(decoded, real_imgs)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as discriminator ground truth\n",
    "        z = torch.randn(batch_x.shape[0], latent_dim)\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(z), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(encoded_imgs.detach()), fake)\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "    batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "    if epoch >= 25 and epoch % 10 == 0:\n",
    "        val = input(\"\\nContinue training? [y/n]: \")\n",
    "        print()\n",
    "        if val in ('y', 'yes'):\n",
    "            val = True\n",
    "            pass\n",
    "        elif val in ('n', 'no'):\n",
    "            break  \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if epoch > 10:\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            generate(n_row=5, batches_done=batches_done, directory=directory)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, d_loss.item(), g_loss.item())\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb952b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40016004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
